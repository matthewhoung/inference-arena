# ============================================================================
# Inference Arena - Environment Configuration Template
# ============================================================================
# Copy this file to .env and fill in your values
# NEVER commit .env to git (it's in .gitignore)
# ============================================================================

# ----------------------------------------------------------------------------
# MinIO Configuration
# ----------------------------------------------------------------------------
# Object storage for model artifacts and Triton model repository

MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# External endpoint for client connections (host:port)
MINIO_EXTERNAL_ENDPOINT=localhost:9000

# S3-compatible endpoint for internal services
MINIO_INTERNAL_ENDPOINT=minio:9000

# Default bucket for model storage
MINIO_BUCKET=models

# ----------------------------------------------------------------------------
# Grafana Configuration
# ----------------------------------------------------------------------------
# Monitoring and visualization dashboard

GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin

# ----------------------------------------------------------------------------
# Prometheus Configuration
# ----------------------------------------------------------------------------
# Metrics collection and time-series database

# Scrape interval (should match experiment.yaml)
PROMETHEUS_SCRAPE_INTERVAL=1s

# Data retention period
PROMETHEUS_RETENTION_DAYS=15d

# ----------------------------------------------------------------------------
# Infrastructure Ports
# ----------------------------------------------------------------------------
# Port mappings for local development

# MinIO
MINIO_API_PORT=9000
MINIO_CONSOLE_PORT=9001

# Monitoring
CADVISOR_PORT=8080
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000

# ----------------------------------------------------------------------------
# Resource Limits (for controlled experiments)
# ----------------------------------------------------------------------------
# These should match experiment.yaml controlled_variables.resources

# CPU cores per container
CONTAINER_VCPU=2

# Memory limit per container (MB)
CONTAINER_MEMORY=4096

# ----------------------------------------------------------------------------
# Experiment Configuration
# ----------------------------------------------------------------------------
# Override experiment.yaml values for local development (optional)

# Path to experiment specification
# EXPERIMENT_CONFIG_PATH=./experiment.yaml

# Enable debug logging
# DEBUG=false

# Results output directory
# RESULTS_DIR=./results

# ----------------------------------------------------------------------------
# Optional: Triton Inference Server
# ----------------------------------------------------------------------------
# Uncomment when deploying Triton

# TRITON_HTTP_PORT=8000
# TRITON_GRPC_PORT=8001
# TRITON_METRICS_PORT=8002
# TRITON_MODEL_REPOSITORY=s3://localhost:9000/models

# ----------------------------------------------------------------------------
# Optional: Load Testing
# ----------------------------------------------------------------------------
# Locust configuration for performance testing

# LOCUST_WEB_PORT=8089
# LOCUST_MASTER_HOST=localhost
# LOCUST_USERS=100
# LOCUST_SPAWN_RATE=10
