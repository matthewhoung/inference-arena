# =============================================================================
# Triton Architecture - Docker Compose Configuration
# =============================================================================
#
# This compose file deploys the Triton architecture with:
# - Init container to download models from MinIO
# - Triton Inference Server (2 vCPU, 4GB memory)
# - Gateway service for HTTP orchestration (2 vCPU, 4GB memory)
#
# Prerequisites:
#   1. Infrastructure must be running:
#      docker compose -f infrastructure/docker-compose.infra.yml up -d
#
#   2. Models and config.pbtxt must be uploaded to MinIO:
#      python infrastructure/minio/init_models.py --force
#
# Usage:
#   docker compose -f architectures/triton/docker-compose.yml up -d
#
# Monitoring:
#   - Gateway: http://localhost:8300/health
#   - Triton HTTP: http://localhost:8000/v2/health/ready
#   - Triton gRPC: localhost:8001
#   - Metrics: Scraped by cAdvisor (visible in Grafana)
#
# Author: Matthew Hong
# Specification Reference: Ch3 Methodology §3.4.3 Triton Architecture
# =============================================================================

services:
  # ===========================================================================
  # Init Container - Download Models from MinIO
  # ===========================================================================
  # Runs once at startup to populate the model repository volume
  # ===========================================================================
  triton-init:
    image: python:3.11-slim
    container_name: inference-arena-triton-init
    command: >
      sh -c "
        pip install --no-cache-dir minio &&
        python /scripts/init_triton_models.py
      "

    # -------------------------------------------------------------------------
    # Volumes
    # -------------------------------------------------------------------------
    volumes:
      - triton-models:/models
      - ./init_triton_models.py:/scripts/init_triton_models.py:ro

    # -------------------------------------------------------------------------
    # Environment Configuration
    # -------------------------------------------------------------------------
    environment:
      MINIO_INTERNAL_ENDPOINT: ${MINIO_INTERNAL_ENDPOINT:-minio:9000}
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      MINIO_BUCKET: ${MINIO_BUCKET:-models}

    # -------------------------------------------------------------------------
    # Network Configuration
    # -------------------------------------------------------------------------
    networks:
      - backend-network

    # -------------------------------------------------------------------------
    # Restart Policy
    # -------------------------------------------------------------------------
    # Init containers should not restart - run once and exit
    # -------------------------------------------------------------------------
    restart: "no"

  # ===========================================================================
  # Triton Inference Server
  # ===========================================================================
  # NVIDIA Triton Inference Server serving ONNX models
  # ===========================================================================
  triton-server:
    image: nvcr.io/nvidia/tritonserver:24.08-py3
    container_name: inference-arena-triton-server
    command: tritonserver --model-repository=/models --log-verbose=1

    # -------------------------------------------------------------------------
    # Port Mapping
    # -------------------------------------------------------------------------
    ports:
      - "8000:8000"  # HTTP inference
      - "8001:8001"  # gRPC inference
      - "8002:8002"  # Prometheus metrics

    # -------------------------------------------------------------------------
    # Volumes
    # -------------------------------------------------------------------------
    # Mount model repository from init container (read-only)
    # -------------------------------------------------------------------------
    volumes:
      - triton-models:/models:ro

    # -------------------------------------------------------------------------
    # Resource Constraints (CPU-only, matching experiment.yaml)
    # -------------------------------------------------------------------------
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4096M
        reservations:
          cpus: '2'
          memory: 4096M

    # -------------------------------------------------------------------------
    # Environment Configuration
    # -------------------------------------------------------------------------
    environment:
      # Force CPU-only inference (no CUDA)
      ORT_DISABLE_ALL_EPS_EXCEPT_CPU: "1"

    # -------------------------------------------------------------------------
    # Network Configuration
    # -------------------------------------------------------------------------
    networks:
      - backend-network

    # -------------------------------------------------------------------------
    # Startup Dependencies
    # -------------------------------------------------------------------------
    # Wait for init container to download models successfully
    # -------------------------------------------------------------------------
    depends_on:
      triton-init:
        condition: service_completed_successfully

    # -------------------------------------------------------------------------
    # Health Check
    # -------------------------------------------------------------------------
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s

    # -------------------------------------------------------------------------
    # Restart Policy
    # -------------------------------------------------------------------------
    restart: unless-stopped

  # ===========================================================================
  # Gateway Service
  # ===========================================================================
  # FastAPI service orchestrating detection → classification pipeline
  # ===========================================================================
  triton-gateway:
    build:
      context: ../..
      dockerfile: architectures/triton/gateway/Dockerfile
    image: inference-arena-triton-gateway:latest
    container_name: inference-arena-triton-gateway

    # -------------------------------------------------------------------------
    # Port Mapping
    # -------------------------------------------------------------------------
    ports:
      - "8300:8300"

    # -------------------------------------------------------------------------
    # Resource Constraints (matching experiment.yaml)
    # -------------------------------------------------------------------------
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4096M
        reservations:
          cpus: '2'
          memory: 4096M

    # -------------------------------------------------------------------------
    # Environment Configuration
    # -------------------------------------------------------------------------
    environment:
      # Triton Configuration
      TRITON_GRPC_ENDPOINT: triton-server:8001
      TRITON_TIMEOUT_SECONDS: "60"

      # Service Configuration
      LOG_LEVEL: INFO
      PORT: 8300
      LABELS_FILE: /app/shared/data/imagenet_labels.txt

    # -------------------------------------------------------------------------
    # Network Configuration
    # -------------------------------------------------------------------------
    networks:
      - backend-network

    # -------------------------------------------------------------------------
    # Startup Dependencies
    # -------------------------------------------------------------------------
    # Wait for Triton server to be healthy before starting
    # -------------------------------------------------------------------------
    depends_on:
      triton-server:
        condition: service_healthy

    # -------------------------------------------------------------------------
    # Health Check
    # -------------------------------------------------------------------------
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8300/health')"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 60s

    # -------------------------------------------------------------------------
    # Restart Policy
    # -------------------------------------------------------------------------
    restart: unless-stopped

# =============================================================================
# Networks
# =============================================================================
# Connect to external backend network created by infrastructure compose file
# =============================================================================
networks:
  backend-network:
    name: inference-arena-backend
    external: true

# =============================================================================
# Volumes
# =============================================================================
# Shared volume for Triton model repository
# =============================================================================
volumes:
  triton-models:
    name: inference-arena-triton-models
